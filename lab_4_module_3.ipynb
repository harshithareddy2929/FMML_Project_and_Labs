{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+MneeJCqwvJEjPTWSCBrV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithareddy2929/FMML_Project_and_Labs/blob/main/lab_4_module_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION 1\n",
        "\n",
        "Instead of using cross-validation, we could also split our dataset into three sets- train, validation, and test set. The training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set. What do you think are the pros/cons of this method ?\n",
        "#ANSWER:\n",
        "\n",
        " Splitting the dataset into three sets (train, validation, and test sets) is a common practice in machine learning and has its own set of pros and cons:\n",
        "\n",
        "#**Pros:**\n",
        "\n",
        "**Independent Evaluation:**\n",
        "\n",
        "The test set provides an independent evaluation of the model's performance. It helps assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "**Model Selection:**\n",
        "\n",
        "The validation set is often used for model selection, hyperparameter tuning, and early stopping. It allows you to experiment with different models and hyperparameters without touching the test set.\n",
        "\n",
        "**Avoiding Overfitting:**\n",
        "\n",
        "By having a separate validation set, you can monitor the model's performance during training and prevent overfitting. If the model performs well on the training set but poorly on the validation set, it might be overfitting.\n",
        "\n",
        "**Final Evaluation:**\n",
        "\n",
        "The test set is used for the final evaluation after all model training and tuning. It provides a reliable estimate of how the model is expected to perform on new, unseen data.\n",
        "\n",
        "#**Cons:**\n",
        "*Reduced Training Data: *\n",
        "\n",
        "With the introduction of a validation set, the actual training data available for model training is reduced. This might be a concern, especially if the dataset is small.\n",
        "\n",
        "*Randomness in Split: *\n",
        "\n",
        "The randomness in the initial split between training and validation sets can lead to variability in model performance. Different splits may result in different models and performance metrics.\n",
        "\n",
        "Data Distribution:\n",
        "\n",
        "If the distribution of data changes over time or varies between different sets, the model may not generalize well. This is especially relevant if the data collection process changes or if there are specific patterns in the data.\n",
        "\n",
        "**Data Leakage:**\n",
        "\n",
        " It's crucial to ensure that there is no information leakage from the validation or test set to the training set. Leakage can occur if there is unintentional overlap or if information about the test set is used during model training.\n",
        "\n",
        "#QUESTION 2\n",
        "\n",
        "Is K-Fold cross validation an exhaustive or non-exhaustive cross validation method ? Justify your answer.\n",
        "#ANSWER\n",
        " K-Fold Cross Validation is a non-exhaustive cross-validation method.\n",
        "\n",
        "In K-Fold Cross Validation, the dataset is divided into K folds, and the model is trained and evaluated K times, each time using a different fold as the test set and the remaining folds as the training set. The final performance metric is often an average or aggregation of the performance metrics obtained in each iteration.\n",
        "\n",
        "#**Justification:**\n",
        "\n",
        "**Not Exhaustive Combination of Folds:**\n",
        "\n",
        "K-Fold Cross Validation does not consider all possible combinations of training and test sets. It uses a subset of the data for training and testing in each iteration, and the process is repeated K times. This is in contrast to exhaustive methods like Leave-One-Out Cross Validation (LOOCV), where every single data point is used as a test set exactly once.\n",
        "\n",
        "**Computational Efficiency:**\n",
        "\n",
        "K-Fold Cross Validation is computationally more efficient than exhaustive methods, especially when the dataset is large. It provides a good balance between computational cost and obtaining a reliable estimate of the model's performance.\n",
        "\n",
        "**Variability in Performance:**\n",
        "\n",
        "The non-exhaustive nature of K-Fold Cross Validation introduces variability in the performance metrics across different folds. This variability can provide insights into the model's stability and robustness.\n",
        "\n",
        "Bias-Variance Trade-Off:\n",
        "\n",
        "K-Fold Cross Validation helps in understanding the bias-variance trade-off by providing multiple estimates of the model's performance on different subsets of the data. This is valuable for assessing how well the model generalizes to different portions of the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "pBj90lwgw1hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wNVwY__03X9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uwBYuhet2ABY"
      }
    }
  ]
}