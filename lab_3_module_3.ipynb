{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBRwQejz6sUogP2dCstq2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithareddy2929/FMML_Project_and_Labs/blob/main/lab_3_module_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION** 1)Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?\n",
        "**ANSWER**:-The Term Frequency-Inverse Document Frequency (TF-IDF) approach generally results in better accuracy than the Bag-of-Words (BoW) model because TF-IDF takes into account not only the frequency of terms in a document (as BoW does) but also considers the importance of each term in the entire corpus. Here are some reasons why TF-IDF can be more effective:\n",
        "\n",
        "1)Term Importance: TF-IDF assigns weights to terms based on how frequently they appear in a document and how rare they are across the entire corpus. Terms that are common in a specific document but rare in the entire corpus are given higher weights. This helps in capturing the unique characteristics of each document.\n",
        "\n",
        "2)Downweighting Common Terms: TF-IDF automatically downweights terms that are common across many documents. Common words like \"the,\" \"and,\" etc., which might be prevalent in many documents but may not carry much meaning, are given lower weights.\n",
        "\n",
        "3)Handling Stop Words: TF-IDF naturally handles common stop words by assigning them lower weights, reducing their impact on the overall representation of the document. In contrast, BoW may treat all words equally, including common stop words.\n",
        "\n",
        "4)Global Information: TF-IDF considers global information about the corpus, providing a more holistic view of the importance of terms. This helps in capturing the semantic meaning of words and their significance across different documents.\n",
        "\n",
        "5)Normalization: TF-IDF normalizes the term frequencies, addressing the issue of document length. This is important because longer documents may have higher term frequencies even if they are not more informative.\n",
        "\n",
        "6)Better Discrimination: TF-IDF helps in better discriminating between documents by emphasizing terms that are unique to each document or have distinctive patterns within the corpus. Reduced Dimensionality:\n",
        "\n",
        "TF-IDF can lead to a more compact and informative representation of documents compared to BoW. This can be beneficial when working with high-dimensional data, as it reduces the risk of overfitting. It's important to note that the effectiveness of TF-IDF versus Bag-of-Words can depend on the specific characteristics of the dataset and the nature of the text classification task. In some cases, BoW might be sufficient, especially if the corpus is small or the task involves capturing local word patterns rather than global term importance. However, in many natural language processing applications, TF-IDF tends to outperform BoW in capturing the semantic meaning and importance of terms."
      ],
      "metadata": {
        "id": "0Nq62NNBCphE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION** 1)Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?\n",
        "**ANSWER**:-The Term Frequency-Inverse Document Frequency (TF-IDF) approach generally results in better accuracy than the Bag-of-Words (BoW) model because TF-IDF takes into account not only the frequency of terms in a document (as BoW does) but also considers the importance of each term in the entire corpus. Here are some reasons why TF-IDF can be more effective:\n",
        "\n",
        "1)Term Importance: TF-IDF assigns weights to terms based on how frequently they appear in a document and how rare they are across the entire corpus. Terms that are common in a specific document but rare in the entire corpus are given higher weights. This helps in capturing the unique characteristics of each document.\n",
        "\n",
        "2)Downweighting Common Terms: TF-IDF automatically downweights terms that are common across many documents. Common words like \"the,\" \"and,\" etc., which might be prevalent in many documents but may not carry much meaning, are given lower weights.\n",
        "\n",
        "3)Handling Stop Words: TF-IDF naturally handles common stop words by assigning them lower weights, reducing their impact on the overall representation of the document. In contrast, BoW may treat all words equally, including common stop words.\n",
        "\n",
        "4)Global Information: TF-IDF considers global information about the corpus, providing a more holistic view of the importance of terms. This helps in capturing the semantic meaning of words and their significance across different documents.\n",
        "\n",
        "5)Normalization: TF-IDF normalizes the term frequencies, addressing the issue of document length. This is important because longer documents may have higher term frequencies even if they are not more informative.\n",
        "\n",
        "6)Better Discrimination: TF-IDF helps in better discriminating between documents by emphasizing terms that are unique to each document or have distinctive patterns within the corpus. Reduced Dimensionality:\n",
        "\n",
        "TF-IDF can lead to a more compact and informative representation of documents compared to BoW. This can be beneficial when working with high-dimensional data, as it reduces the risk of overfitting. It's important to note that the effectiveness of TF-IDF versus Bag-of-Words can depend on the specific characteristics of the dataset and the nature of the text classification task. In some cases, BoW might be sufficient, especially if the corpus is small or the task involves capturing local word patterns rather than global term importance. However, in many natural language processing applications, TF-IDF tends to outperform BoW in capturing the semantic meaning and importance of terms."
      ],
      "metadata": {
        "id": "1ewHMPBpC5YM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION** 3)Write the pros and cons of stemming and lemmatization.\n",
        "**ANSWER**:-\n",
        "STEMMING:-\n",
        "\n",
        "PROS:-\n",
        "\n",
        "1)Computational Efficiency: Stemming is computationally less expensive compared to lemmatization. It involves simpler rules to chop off prefixes or suffixes, making it faster for large datasets.\n",
        "\n",
        "2)Reduction of Features: Stemming reduces words to their root or base form, which can help in reducing the dimensionality of the data. This can be particularly useful in text processing and machine learning applications.\n",
        "\n",
        "3)Improved Recall: Stemming can improve recall in information retrieval tasks because it groups together words that share a common root. This can be beneficial when searching for information.\n",
        "\n",
        "CONS:-\n",
        "\n",
        "1)Over-Stemming: Stemming may sometimes result in overly aggressive word reduction, leading to the loss of meaning. This phenomenon is known as over-stemming.\n",
        "\n",
        "2)Loss of Discriminative Power: Since stemming involves removing prefixes or suffixes without understanding the context, it may lead to the loss of discriminative power in certain cases.\n",
        "\n",
        "3)Not Language-Specific: Stemming algorithms are generally language-agnostic, which means they may not be optimized for specific languages, and their effectiveness can vary across different languages\n",
        "\n",
        "LEMMATIZATION:-\n",
        "\n",
        "PROS:-\n",
        "\n",
        "1)Linguistic Accuracy: Lemmatization provides a linguistically accurate base or root form of a word, considering the context and meaning. This helps in preserving the semantics of the text.\n",
        "\n",
        "2)Improved Readability: Since lemmatization produces real words, the output is often more readable and interpretable compared to stemming, which might generate non-words.\n",
        "\n",
        "3)Better Precision: Lemmatization generally maintains better precision compared to stemming because it produces valid words, reducing the risk of ambiguity.\n",
        "\n",
        "CONS:-\n",
        "\n",
        "1)Computational Cost: Lemmatization is computationally more intensive than stemming. It involves dictionary lookups and understanding the context of words, making it slower, especially for large datasets.\n",
        "\n",
        "2)Increased Dimensionality: The output of lemmatization may result in a larger number of unique words compared to stemming. This can potentially increase the dimensionality of the data.\n",
        "\n",
        "3)Language-Specific Challenges: Lemmatization requires language-specific resources and rules. Implementing effective lemmatization for multiple languages may involve more complexity"
      ],
      "metadata": {
        "id": "O829Wd8yDKU7"
      }
    }
  ]
}