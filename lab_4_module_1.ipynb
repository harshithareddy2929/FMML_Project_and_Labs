{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOarvunrAvfmj/moleuT2Ri",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithareddy2929/FMML_Project_and_Labs/blob/main/lab_4_module_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Questions**:\n",
        "# 1.Experiment with different transformation matrices and check the accuracy\n",
        "# 2.Will the same transform used for these two features also work for other features?"
      ],
      "metadata": {
        "id": "_fCJjl1TJITK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "def apply_transformation(image, transformation_matrix):\n",
        "    h, w = image.shape[:2]\n",
        "    return cv2.warpAffine(image, transformation_matrix, (w, h))\n",
        "\n",
        "def random_rotation_matrix(max_angle=30):\n",
        "    angle = np.random.uniform(-max_angle, max_angle)\n",
        "    return cv2.getRotationMatrix2D((0, 0), angle, 1.0)\n",
        "\n",
        "def random_translation_matrix(max_px=10):\n",
        "    tx = np.random.randint(-max_px, max_px)\n",
        "    ty = np.random.randint(-max_px, max_px)\n",
        "    return np.array([[1, 0, tx], [0, 1, ty]], dtype=np.float32)\n",
        "\n",
        "def random_scaling_matrix(min_scale=0.8, max_scale=1.2):\n",
        "    sx = np.random.uniform(min_scale, max_scale)\n",
        "    sy = np.random.uniform(min_scale, max_scale)\n",
        "    return np.array([[sx, 0, 0], [0, sy, 0]], dtype=np.float32)\n",
        "augmented_images = []\n",
        "for image in train_images:\n",
        "    transformation_matrix = random_rotation_matrix()  # Change this to other transformations\n",
        "    augmented_image = apply_transformation(image, transformation_matrix)\n",
        "    augmented_images.append(augmented_image)\n",
        "\n",
        "train_images_augmented = np.array(augmented_images)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images_augmented, train_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "KGzwnt97JAsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same transformation used for one feature or type of data may not necessarily work optimally for other features or data types. The choice of transformation should consider the unique characteristics and requirements of each data feature or type. Here's why:\n",
        "\n",
        "**Data Characteristics**: Different data types have different characteristics. For example, images are two-dimensional grids of pixels, and geometric transformations like rotation, scaling, and translation are appropriate because they preserve spatial relationships. Text data, on the other hand, is sequential and requires different types of transformations like shuffling words or adding noise.\n",
        "\n",
        "**Domain Knowledge**: Domain-specific knowledge often dictates which transformations are appropriate. For instance, in medical imaging, it may be crucial to use specific transformations that respect anatomical constraints, while in natural language processing, you may need text-specific transformations to preserve the meaning of sentences.\n",
        "\n",
        "**Task-Specific Requirements**: The choice of transformations can be task-dependent. For image classification, you might apply geometric transformations to augment your training data. However, for sentiment analysis of text, you might introduce noise or perturbations to make the model more robust to variations in text inputs.\n",
        "\n",
        "**Experimentation**: It's essential to experiment with different transformations and evaluate their impact on model performance for each specific feature or data type. What works best may vary between datasets and tasks.\n",
        "\n",
        "**Feature Engineering**: For structured data, such as tabular data, feature engineering techniques are often more relevant than traditional data augmentations used for images or text. Feature engineering can include creating interaction terms, scaling, or encoding categorical variables.\n",
        "\n",
        "In summary, while data augmentation is a valuable technique for improving model generalization, the choice of transformations should be tailored to the characteristics of the data and the task at hand. It's common to have different sets of transformations for different types of data within the same machine learning project. Therefore, it's essential to consider the unique requirements and characteristics of each data feature or type when designing your data augmentation or preprocessing pipeline."
      ],
      "metadata": {
        "id": "GXVJJvL2JQ8M"
      }
    }
  ]
}