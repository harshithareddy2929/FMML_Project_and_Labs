{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYScsm1hEwQN3jHOhVNDxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithareddy2929/FMML_Project_and_Labs/blob/main/lab_2_module_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION** 1)In the section on Decision boundaries, you must have seen that we ran the KNN algorithm twice: first with the weights set to 'uniform' and then set to 'distance'. Find out the difference between these two.\n",
        "\n",
        "**ANSWER**:-In the context of the k-nearest neighbors (KNN) algorithm, the parameter you're referring to—'uniform' versus 'distance'—is related to how the algorithm assigns weights to the neighboring points when making predictions.\n",
        "\n",
        "Uniform Weighting:\n",
        "\n",
        "When the parameter is set to 'uniform', all neighboring points have equal weight in the decision-making process. In other words, each of the k-nearest neighbors contributes equally to the prediction, regardless of their distance from the point being predicted.\n",
        "\n",
        "Distance Weighting:\n",
        "\n",
        "When the parameter is set to 'distance', closer neighbors have more influence on the prediction, while farther neighbors have less influence. The idea is that points that are closer to the query point are likely to be more relevant, so their contribution to the prediction is weighted more heavily. In summary, the difference lies in how the algorithm considers the importance of neighboring points. 'Uniform' gives equal weight to all neighbors, while 'distance' gives more weight to closer neighbors. The choice between these options can impact the model's behavior and performance, and the optimal choice often depends on the specific characteristics of the dataset"
      ],
      "metadata": {
        "id": "FvCkfY1oGW_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION** 2)What do you think could be the drawbacks of using KNN ?\n",
        "**ANSWER**:-While the k-nearest neighbors (KNN) algorithm is simple and intuitive, it also has several drawbacks and limitations that may make it less suitable for certain scenarios. Here are some drawbacks of using KNN:\n",
        "\n",
        "1)Computational Cost: As the size of the dataset increases, the computational cost of KNN also increases. This is because, during prediction, the algorithm needs to calculate the distances between the query point and all data points in the dataset.\n",
        "\n",
        "2)Memory Usage: KNN requires storing the entire dataset in memory, as it needs to access and compare each data point during prediction. For large datasets, this can be memory-intensive.\n",
        "\n",
        "3)Sensitivity to Outliers: KNN is sensitive to outliers and noise in the data. Outliers can significantly impact the decision boundaries and lead to incorrect predictions.\n",
        "\n",
        "4)Curse of Dimensionality: In high-dimensional spaces, the concept of proximity becomes less meaningful. This is known as the curse of dimensionality, and it can lead to degraded performance of KNN as the number of features increases.\n",
        "\n",
        "5)Need for Feature Scaling: KNN is sensitive to the scale of features. Features with larger scales can dominate the distance calculation, leading to biased results. Therefore, it's often necessary to normalize or standardize the features.\n",
        "\n",
        "6)Choosing an Optimal K: The performance of KNN is highly dependent on the choice of the parameterk (the number of neighbors). Selecting an appropriate k value can be challenging and may require experimentation.\n",
        "\n",
        "7)Imbalanced Datasets: KNN may struggle with imbalanced datasets where one class significantly outnumbers the others. The majority class can dominate the predictions, leading to poor performance on minority classes.\n",
        "\n",
        "8)No Model Representation: KNN does not learn an explicit model during training, making it less interpretable. It doesn't provide insights into which features are most important for predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "xDZUeyiWGciY"
      }
    }
  ]
}