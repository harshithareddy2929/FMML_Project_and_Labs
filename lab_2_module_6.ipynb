{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWzgarB8ClV/rB9/uqoQUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithareddy2929/FMML_Project_and_Labs/blob/main/lab_2_module_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercises\n",
        "#Q1.Try out these loss functions on regression tasks, and try to understand the model performance based on the loss function.\n",
        "#1. Regression Loss Functions:\n",
        "\n",
        "##Mean Squared Error (MSE):"
      ],
      "metadata": {
        "id": "NH8Vi9C-r0IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = np.mean((y_actual - y_pred) ** 2)\n",
        "print(\"MSE is: {}\".format(mse))\n",
        "print(\"RMSE is: {}\".format(np.sqrt(mse)))\n"
      ],
      "metadata": {
        "id": "1kArxOktse4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mean Absolute Error (MAE):"
      ],
      "metadata": {
        "id": "ZU_PhDQKssKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae = np.mean(np.abs(y_actual - y_pred))\n",
        "print(\"MAE is: {}\".format(mae))\n"
      ],
      "metadata": {
        "id": "8k0lMOf6svxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##observations:\n",
        "MSE penalizes larger errors more heavily due to squaring.\n",
        "RMSE is used for better interpretation of the error in the same units as the data.\n",
        "MAE is robust to outliers but can be non-differentiable.\n",
        "#2. Classification Loss Functions:\n",
        "##Maximum Likelihood Estimation (MLE):\n",
        "MLE aims to maximize the likelihood of the model parameters given the observed data.\n",
        "##Entropy and Cross-Entropy:"
      ],
      "metadata": {
        "id": "v2qpT4kXs4sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_actual):\n",
        "    if y_actual == 1:\n",
        "        return -np.log(y_pred)\n",
        "    return -np.log(1 - y_pred)\n"
      ],
      "metadata": {
        "id": "0qjP_AiIs66h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##observations:\n",
        ". MLE and Cross-Entropy are closely related, both aiming to maximize the likelihood of the predicted probabilities.\n",
        "# KNN-Regression:"
      ],
      "metadata": {
        "id": "bg16FRkztf16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_regression(k, x_train, y_train, x_test, y_test):\n",
        "    model = neighbors.KNeighborsRegressor(n_neighbors=k)\n",
        "    model.fit(x_train, y_train)\n",
        "    pred = model.predict(x_test)\n",
        "    error = sqrt(mean_squared_error(y_test, pred))\n",
        "    rmse_val.append(error)\n",
        "\n",
        "rmse_val = []\n",
        "for k in range(1, 21):\n",
        "    knn_regression(k, x_train, y_train, x_test, y_test)\n",
        "\n",
        "figure = plt.figure(figsize=(10, 10))\n",
        "plt.plot(np.arange(1, 21), rmse_val)\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "rmse_val = np.asarray(rmse_val)\n",
        "print(\"Minimum error {} is at k = {}\".format(np.min(rmse_val), np.argmin(rmse_val)))\n"
      ],
      "metadata": {
        "id": "vzI626kr1JIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observations:\n",
        ".The KNN-regression is using RMSE as the loss function.\n",
        "\n",
        "\n",
        ".The plot helps visualize how the choice of 'k' impacts the model's performance.\n",
        "\n",
        "#Q2.Explore other loss functions and try to understand when and why they are used.\n",
        "There are several other loss functions used in machine learning, each with its specific characteristics and use cases. Here are a few additional loss functions along with insights into when and why they are used:\n",
        "\n",
        "#1. Huber Loss:\n",
        "Huber Loss is a combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE). It is less sensitive to outliers than MSE and more robust."
      ],
      "metadata": {
        "id": "y5J8dlVEt9XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def huber_loss(y_actual, y_pred, delta=1.0):\n",
        "    abs_diff = np.abs(y_actual - y_pred)\n",
        "    loss = np.where(abs_diff < delta, 0.5 * (abs_diff ** 2), delta * (abs_diff - 0.5 * delta))\n",
        "    return np.mean(loss)\n"
      ],
      "metadata": {
        "id": "nwomPJmVuxp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use Case:\n",
        "Huber Loss is suitable when dealing with datasets containing outliers. It provides a compromise between the robustness of MAE and the sensitivity of MSE.\n",
        "\n",
        "#2. Quantile Loss:\n",
        "Quantile Loss is used in quantile regression, where the goal is to predict the conditional quantiles of the target variable. It is defined for a given quantile level 'tau.'"
      ],
      "metadata": {
        "id": "hj2RprsXu3Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantile_loss(y_actual, y_pred, tau):\n",
        "    error = y_actual - y_pred\n",
        "    loss = np.where(error >= 0, tau * error, (tau - 1) * error)\n",
        "    return np.mean(loss)\n"
      ],
      "metadata": {
        "id": "l7cGzoHgu8qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use Case:\n",
        "Quantile Loss is useful when interested in predicting different quantiles of the target variable's distribution, providing a more comprehensive understanding of uncertainty.\n",
        "\n",
        "#3. Log-Cosh Loss:\n",
        "Log-Cosh Loss is a smooth approximation of Huber Loss. It is less sensitive to outliers than MSE and more robust."
      ],
      "metadata": {
        "id": "COcD86AbvCzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_cosh_loss(y_actual, y_pred):\n",
        "    log_cosh = np.log(np.cosh(y_pred - y_actual))\n",
        "    return np.mean(log_cosh)\n"
      ],
      "metadata": {
        "id": "BPfE7iQUvN7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use Case:\n",
        " Log-Cosh Loss is suitable when a balance between the smoothness of the loss function and resistance to outliers is desired.\n",
        "\n",
        "#4. Poisson Loss:\n",
        "Poisson Loss is used for regression tasks where the target variable follows a Poisson distribution, such as count data."
      ],
      "metadata": {
        "id": "7mxFLPXEvSMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def poisson_loss(y_actual, y_pred):\n",
        "    return np.mean(y_pred - y_actual * np.log(y_pred))\n"
      ],
      "metadata": {
        "id": "4RrbFBTkvc4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use Case:\n",
        "Poisson Loss is appropriate when modeling count data, such as the number of events occurring in a fixed interval of time or space.\n",
        "\n",
        "#5. Hinge Loss:\n",
        "Hinge Loss is commonly used for Support Vector Machines (SVMs) in classification tasks. It encourages correct classification by penalizing misclassifications."
      ],
      "metadata": {
        "id": "tCYH4DHpvfCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hinge_loss(y_actual, y_pred):\n",
        "    return np.mean(np.maximum(0, 1 - y_actual * y_pred))\n"
      ],
      "metadata": {
        "id": "rT8ntfusvoSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use Case:\n",
        " Hinge Loss is suitable for binary classification tasks, especially when training support vector machines\n",
        "#Conclusion:\n",
        "The choice of a particular loss function depends on the nature of the problem, the characteristics of the data, and the goals of the modeling task. It's essential to consider the specific requirements of the problem when selecting a loss function for training a machine learning model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gpW7rmRHvsIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3.Try out KNN-regression on other datasets see which values of K give the best results.\n",
        "\n",
        "To try KNN-regression on other datasets and determine the best values of 'k', we can follow a similar procedure with a different dataset. Here, I'll demonstrate using the famous California Housing Prices dataset:"
      ],
      "metadata": {
        "id": "_9SJ95ZLw-AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Function to perform KNN-regression and calculate RMSE\n",
        "def knn_regression(k, X_train, y_train, X_test, y_test):\n",
        "    model = KNeighborsRegressor(n_neighbors=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    error = sqrt(mean_squared_error(y_test, pred))\n",
        "    return error\n",
        "\n",
        "# Try different values of k\n",
        "k_values = list(range(1, 21))\n",
        "rmse_values = []\n",
        "\n",
        "for k in k_values:\n",
        "    rmse = knn_regression(k, X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "    rmse_values.append(rmse)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, rmse_values, marker='o')\n",
        "plt.title('KNN Regression on California Housing Prices Dataset')\n",
        "plt.xlabel('Number of Neighbors (k)')\n",
        "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the best value of k\n",
        "best_k = k_values[np.argmin(rmse_values)]\n",
        "print(\"The best value of k is:\", best_k)\n"
      ],
      "metadata": {
        "id": "VggFIOgKxOet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, I'm using the California Housing dataset, splitting it into training and testing sets, scaling the features, and then performing KNN-regression for different values of 'k'. The results are plotted to visualize how RMSE changes with different values of 'k', and the best value of 'k' is printed"
      ],
      "metadata": {
        "id": "CqAoc6plxaFL"
      }
    }
  ]
}